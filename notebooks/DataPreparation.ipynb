{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "fc3712ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets into prompts\n",
    "\n",
    "# Datasets:\n",
    "# Curation\n",
    "# Amazon prod descs: It's the McAuley data (the metadata part)\n",
    "# Wikihow\n",
    "# FactualNLG\n",
    "# DialogSum\n",
    "\n",
    "\n",
    "import jsonlines, json\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "SUMMARIZE_PROMPT = \"{:}\\n\\nGenerate a summary:\\n\"\n",
    "\n",
    "FACTUALNLG_PROMPT = \"\"\"Given the document below, determine if the summary is factually consistent with the document. \n",
    "Document: {:}\n",
    "Summary: {:}\n",
    "Is the summary consistent with the document? First provide detailed reasoning, then answer \"Yes\" or \"No\".\n",
    "Reasoning:\"\"\"\n",
    "\n",
    "DIALOGSUM_PROMPT = \"Conversation:\\n{:}\\n\\nNow write a summary of the conversation:\\n\"\n",
    "\n",
    "WIKIHOW_PROMPT = \"Can you tell me {:}?\"\n",
    "\n",
    "\n",
    "\n",
    "list_of_names = [\"James\", \"Robert\", \"Michael\", \"William\", \"David\", \"Richard\", \"Adam\", \"Charles\", \"Jonathan\", \"Daniel\", \"Alexander\", \"Mary\", \"Susan\", \"Jennifer\", \"Margaret\", \"Ruth\", \"Sharon\", \"Barbara\", \"Elizabeth\", \"Paula\", \"Karol\", \"Melissa\", \"Louise\", \"Rachel\", \"Zero\", \"Nova\", \"Cohere\", \"Deenah\", \"Anna\", \"Olivia\", \"Woodrow\", \"Jennette\", \"Terence\", \"Skylar\", \"Karen\", \"Marie\", \"Blake\", \"Resnick\", \"Sherlock\", \"Sean\", \"Matt\", \"Jessica\", \"Hannah\", \"Grace\", \"Catherine\", \"Frida\", \"Sebastian\", \"Simon\", \"Alice\", \"Jose\", \"Marcos\", \"Barbara\", \"Jennifer\", \"Mark\", \"Traci\", \"Sam\", \"Ben\", \"Jack\", \"Jane\", \"Joseph\", \"Max\", \"Jennifer\", \"Lisa\", \"Barbara\", \"Jose\", \"Marcos\", \"Charles\", \"Marion\"] \n",
    "\n",
    "def process_amazon(path, limit, cfg):\n",
    "\n",
    "    gs = gcsfs.GCSFileSystem(\n",
    "    )\n",
    "\n",
    "    data = pd.read_parquet(path)\n",
    "    \n",
    "    return list(data.prompt[:limit]), list(data.completion[:limit])\n",
    "\n",
    "def process_macsum(path, limit, cfg):\n",
    "    with open(path) as f:\n",
    "        macdoc = json.load(f)[:limit]\n",
    "        \n",
    "    return [SUMMARIZE_PROMPT.format(\"\\n\".join(row['source'])) for row in macdoc]\n",
    "\n",
    "def process_curation(path, limit, cfg):\n",
    "    exclude_lens = [898, 792]\n",
    "    exclude_strs = ['TRY LAW360 FREE FOR SEVEN DAYS']\n",
    "    def filter_art(art, summ):\n",
    "        if len(art) in exclude_lens:\n",
    "            return False\n",
    "        for exclude_str in exclude_strs:\n",
    "            if exclude_str in art:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    curation = pd.read_csv(path)\n",
    "    \n",
    "    articles = curation.prompt\n",
    "    summs = curation.completion\n",
    "    \n",
    "    samples = [(SUMMARIZE_PROMPT.format(art), summ) for art, summ in zip(articles, summs) if len(art) > cfg['min_chars'] and len(art) < cfg['max_chars'] and filter_art(art, summ)]\n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    return list(zip(*samples[:limit]))\n",
    "\n",
    "def process_wikihow(path, limit, cfg):\n",
    "    wikihow = pd.read_csv(path)\n",
    "    \n",
    "    titles = [title for title in wikihow.title.unique() if isinstance(title, str) and not title[-1].isnumeric()]\n",
    "    np.random.shuffle(titles)\n",
    "    titles = titles[:limit]\n",
    "    \n",
    "    refs = [\"\".join(wikihow.where(wikihow.title == title).dropna().headline.unique()) for title in titles]\n",
    "    \n",
    "    \n",
    "    prompts =  [WIKIHOW_PROMPT.format(title.lower()) for title in titles ]\n",
    "    return prompts, refs\n",
    "\n",
    "def process_dialogsum(path, limit, cfg):\n",
    "    samples = []\n",
    "    with jsonlines.open(path) as reader:\n",
    "        for row in reader:\n",
    "            names = np.random.choice(list_of_names, size=3)\n",
    "            if \"When she told me that she would marry no man but Dick\" in row['dialogue']:\n",
    "                continue\n",
    "            prompt = DIALOGSUM_PROMPT.format(row['dialogue']).replace('#Person1#', names[0]).replace('#Person2#', names[1]).replace('#Person3#', names[2])\n",
    "            summ = row['summary1'].replace('#Person1#', names[0]).replace('#Person2#', names[1]).replace('#Person3#', names[2])\n",
    "            samples.append((prompt, summ))\n",
    "            if len(samples) >= limit:\n",
    "                return list(zip(*samples))\n",
    "            \n",
    "def process_factualnlg(path, limit, cfg):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    prompts = np.random.choice([FACTUALNLG_PROMPT.format(row['doc'], row['summary']) for row in data if row['label'] == 1 or 'entity_modification' in row['edit_types']], limit)\n",
    "    refs = [None] * len(prompts)\n",
    "    return list(prompts), refs\n",
    "\n",
    "configs = {\n",
    "    'amazon': {\n",
    "        'input_data': AMAZON_DIR,\n",
    "        'processor': process_amazon,\n",
    "        'limit': 150\n",
    "    },\n",
    "    'curation': {\n",
    "#         'input_data': '../granular-eval/data/curation-corpus/curation-corpus-articles.csv',\n",
    "        'input_data': CURATION_DIR,\n",
    "        'processor': process_curation,\n",
    "        'limit': 150,\n",
    "        \"min_chars\": 700,\n",
    "        \"max_chars\": 2500\n",
    "    },\n",
    "    'wikihow': {\n",
    "        'input_data': '../granular-eval/data/wikihow/wikihowSep.csv',\n",
    "        'processor': process_wikihow,\n",
    "        'limit': 150\n",
    "    },\n",
    "#     \"dialogsum\": {\n",
    "#         'input_data': '../granular-eval/data/dialogsum/DialogSum_Data/dialogsum.test.jsonl',\n",
    "#         'processor': process_dialogsum,\n",
    "#         'limit': 150\n",
    "#     },\n",
    "#     \"factualnlg\": {\n",
    "#         'input_data': '../granular-eval/data/factualnlg/data/summedits/summedits_news.json',\n",
    "#         'processor': process_factualnlg,\n",
    "#         'limit': 150\n",
    "#     }\n",
    "}\n",
    "\n",
    "\n",
    "#     'macsum': {\n",
    "#         'input_data': '../granular-eval/data/MACSum/dataset/macdoc/test.json',\n",
    "#         'processor': process_macsum,\n",
    "#         'limit': 10\n",
    "#     },\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "fb94043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "refs = []\n",
    "for slug, cfg in configs.items():\n",
    "    curr_prompts, curr_refs = cfg['processor'](cfg['input_data'], cfg['limit'], cfg)\n",
    "    for ix, (prompt, ref) in enumerate(zip(curr_prompts, curr_refs)):\n",
    "        sample_obj = {\n",
    "            'dataset': slug,\n",
    "            'prompt': prompt,\n",
    "            'sample_ix': ix\n",
    "        }\n",
    "        prompts.append(sample_obj)\n",
    "        refs.append({**sample_obj, 'model': 'refs', 'response': ref})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "06a8f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../results/batch_v1/prompts_v2.jsonl','w') as writer:\n",
    "    writer.write_all(prompts)\n",
    "with jsonlines.open('../results/batch_v1/output_v2_refs.jsonl','w') as writer:\n",
    "    writer.write_all(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4764db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Cohere continuations from given input file\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "with jsonlines.open('../results/batch_controlled/prompts_augmented_v2.jsonl') as reader:\n",
    "    prompts = list(reader)\n",
    "\n",
    "import cohere\n",
    "from cohere import CohereAPIError, CohereConnectionError\n",
    "\n",
    "API_KEY = os.environ.get('COHERE_API_KEY')\n",
    "  \n",
    "co = cohere.Client(API_KEY)\n",
    "\n",
    "responses_command = []\n",
    "\n",
    "\n",
    "\n",
    "responses_command = co.batch_generate(\n",
    "    model = 'command-light-nightly',\n",
    "#         model = 'command-nightly',\n",
    "    prompts=[p['prompt'].strip() for p in prompts],\n",
    "    return_exceptions=True,\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,\n",
    "    num_generations=1,\n",
    ")\n",
    "\n",
    "\n",
    "if len([resp for resp in responses_command if isinstance(resp, CohereError) or isinstance(resp, CohereConnectionError)]) > 0:\n",
    "    print('Some samples failed! You may want to try again')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# output_command = [{**sample_obj, 'model': 'command_52B_20230607', 'response': (resp[0].text if not isinstance(resp, CohereAPIError) else None)} for sample_obj, resp in zip(prompts, responses_command)]\n",
    "output_command = [{**sample_obj, 'model': 'command_6Bpref_v14.7_20230817', 'response': (resp[0].text if not isinstance(resp, CohereError) else None)} for sample_obj, resp in zip(prompts, responses_command)]\n",
    "      \n",
    "with jsonlines.open('../results/batch_controlled/output_augmented_v2_command6Bpref.jsonl','w') as writer:\n",
    "    writer.write_all(output_command)\n",
    "    \n",
    "print(len(prompts), len(output_command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53fbe403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'curation': 150, 'wikihow': 150})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataset count\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "dataset_counter = Counter()\n",
    "\n",
    "for row in prompts:\n",
    "    dataset_counter[row['dataset']] += 1\n",
    "    \n",
    "dataset_counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d2320ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duped inputs (FactualNLG has some)\n",
    "\n",
    "with jsonlines.open('../results/batch_v1/prompts_merged.jsonl') as reader:\n",
    "    prompts = list(reader)\n",
    "    \n",
    "prompt_count = Counter()\n",
    "\n",
    "for row in prompts:\n",
    "#     if row['dataset'] == 'factualnlg':\n",
    "    prompt_count[row['prompt']] +=1\n",
    "    \n",
    "duped_prompts = [prompt for prompt, count in prompt_count.items() if count > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c6b43439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command52B 750\n",
      "command6B 750\n",
      "refs 600\n",
      "falcon40 750\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../granular-eval/batch_v1/archive/output_mpt30instruct.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m system \u001b[38;5;129;01min\u001b[39;00m systems:\n\u001b[1;32m      5\u001b[0m     sys_output_merged \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mjsonlines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../granular-eval/batch_v1/archive/output_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msystem\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m      7\u001b[0m         rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(reader)\n\u001b[1;32m      8\u001b[0m     rows \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m row \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: system, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrow} \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows]\n",
      "File \u001b[0;32m~/cohere/notebooks/sandbox/lib/python3.10/site-packages/jsonlines/jsonlines.py:627\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(file, mode, loads, dumps, compact, sort_keys, flush)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m Reader \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m Writer\n\u001b[1;32m    626\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 627\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    629\u001b[0m     loads\u001b[38;5;241m=\u001b[39mloads,\n\u001b[1;32m    630\u001b[0m     dumps\u001b[38;5;241m=\u001b[39mdumps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     flush\u001b[38;5;241m=\u001b[39mflush,\n\u001b[1;32m    634\u001b[0m )\n\u001b[1;32m    635\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../granular-eval/batch_v1/archive/output_mpt30instruct.jsonl'"
     ]
    }
   ],
   "source": [
    "# Merge outputs from multiple prompt batches\n",
    "# all_outputs_merged = []\n",
    "\n",
    "for system in systems:\n",
    "    sys_output_merged = []\n",
    "    with jsonlines.open(f'../results/batch_v1/archive/output_{system}.jsonl') as reader:\n",
    "        rows = list(reader)\n",
    "    rows = [row if 'model' in row else {'model': system, **row} for row in rows]\n",
    "    \n",
    "    for row in rows:\n",
    "        if row['dataset'] in datasets_to_skip_v1:\n",
    "            continue\n",
    "        if row['response'] is not None:\n",
    "            sys_output_merged.append(row)\n",
    "            \n",
    "    with jsonlines.open(f'../results/batch_v1/archive/output_v2_{system}.jsonl') as reader:\n",
    "        rows = list(reader)\n",
    "    rows = [row if 'model' in row else {'model': system, **row} for row in rows]\n",
    "    \n",
    "    for row in rows:\n",
    "        if row['dataset'] not in datasets_to_skip_v1:\n",
    "            continue\n",
    "        if row['response'] is not None:\n",
    "            sys_output_merged.append(row)\n",
    "            \n",
    "    with jsonlines.open(f'../results/batch_v1/output_merged_{system}.jsonl', 'w') as writer:\n",
    "        writer.write_all(sys_output_merged)\n",
    "    print(system, len(sys_output_merged))\n",
    "        \n",
    "        \n",
    "sys_output_merged = []\n",
    "\n",
    "with jsonlines.open(f'../results/batch_v1/prompts.jsonl') as reader:\n",
    "    rows = list(reader)\n",
    "# rows = [row if 'model' in row else {'model': system, **row} for row in rows]\n",
    "\n",
    "for row in rows:\n",
    "    if row['dataset'] in datasets_to_skip_v1:\n",
    "        continue\n",
    "#     if row['response'] is not None:\n",
    "    sys_output_merged.append(row)\n",
    "\n",
    "with jsonlines.open(f'../results/batch_v1/prompts_v2.jsonl') as reader:\n",
    "    rows = list(reader)\n",
    "# rows = [row if 'model' in row else {'model': system, **row} for row in rows]\n",
    "\n",
    "for row in rows:\n",
    "    if row['dataset'] not in datasets_to_skip_v1:\n",
    "        continue\n",
    "#     if row['response'] is not None:\n",
    "\n",
    "    sys_output_merged.append(row)\n",
    "\n",
    "with jsonlines.open(f'../results/batch_v1/prompts_merged.jsonl', 'w') as writer:\n",
    "    writer.write_all(sys_output_merged)\n",
    "    \n",
    "print(len(sys_output_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "29868067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon  has  100  samples\n",
      " .. of which  0  are duplicate\n",
      "curation  has  100  samples\n",
      " .. of which  0  are duplicate\n",
      "wikihow  has  100  samples\n",
      " .. of which  0  are duplicate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "825"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate batches to be annotated\n",
    "\n",
    "# \n",
    "\n",
    "import jsonlines\n",
    "from collections import defaultdict\n",
    "from itertools import combinations, product\n",
    "from math import ceil\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "random.seed(12)\n",
    "np.random.seed(12)\n",
    "\n",
    "\n",
    "#### Settings begins\n",
    "\n",
    "systems = [\n",
    "#     'command52B',\n",
    "#     'command6B',\n",
    "    'command52Bpref',\n",
    "    'command6Bpref',\n",
    "    #     'refs',\n",
    "    'falcon40',\n",
    "    'mpt30instruct',\n",
    "#     'mpt7',\n",
    "#     'vicuna30uncensored',\n",
    "]\n",
    "\n",
    "# Part I - non augmented, used refs for distractors\n",
    "# Main batches started at offset=3:\n",
    "# pilot v5: offset 3, n=1 (inc 2x repeats)\n",
    "# pilot v6: offset 4, n=1 (inc 4x repeats) -> 70\n",
    "# main 1:   offset 5, n=19(inc 4x repeats) -> 1330\n",
    "# main 2:   offset 24, n=20 (inc 0 repeats) -> 1000\n",
    "# main 3:   offset 44, n=20 (inc 0 repeats) -> 1000\n",
    "# main 4:   offset 64, n=20 (inc 0 repeats) -> 1000\n",
    "\n",
    "# Part II - augmented, used all models for distractors\n",
    "# controlled pilot 1: offset 100, n=1, inc 0 repeats # bug: repeated pairing\n",
    "# controlled main 1: offset 100, n=25, inc 4 repeats -> 1125 = (10 combos+1distractor + 4repeats) * 3 datasets * 25\n",
    "# controlled main 2: offset 125, n=25, 0 repeats -> (10+1) * 3 * 25 = 825\n",
    "# controlled main 3: offset 75, n=25, 0 repeats -> (10+1) * 3 * 25 = 825\n",
    "# controlled main 3: offset 50, n=25, 0 repeats -> (10+1) * 3 * 25 = 825\n",
    "\n",
    "NUM_SAMPLES = 25 #NOTE: use multiples of 5 to ensure each model/preamble type is balanced\n",
    "OFFSET = 50\n",
    "NUM_PAIRS_PER_INSTANCE=5\n",
    "NUM_ANNOTATOR_AGREEMENT_SAMPLES = 0 # How many times to duplicate one of the rows, for agreement checking?\n",
    "BATCH_NAME = 'confound_main_4'\n",
    "\n",
    "USE_REFS_MODEL_FOR_DISTRACTORS = False # Set to True to use the same model for distractors each time\n",
    "PROMPT_KEY = 'original_prompt' # set to 'prompt' for non-augmented data\n",
    "REFS_MODEL = 'command_52Bpref_v14.7_20230817' # set to 'refs' for non augmented data\n",
    "\n",
    "# datasets_to_skip_v1 = ['wikihow','curation']\n",
    "\n",
    "#### Settings Ends\n",
    "\n",
    "def normalise_response(resp):\n",
    "    return resp.replace('<s>','').replace('</s>','').replace('\"','\\\"').strip()\n",
    "\n",
    "\n",
    "samples_by_dataset_by_sampleix = defaultdict(lambda: defaultdict(list))\n",
    "all_outputs_merged = []\n",
    "\n",
    "# for system in systems:\n",
    "#     prompts_so_far = set()\n",
    "#     with jsonlines.open(f'../results/batch_v1/output_merged_{system}.jsonl') as reader:\n",
    "\n",
    "with jsonlines.open(f'../results/batch_controlled/output_augmented_merged.jsonl') as reader:\n",
    "    rows = list(reader)\n",
    "rows = [row if 'model' in row else {'model': system, **row} for row in rows]\n",
    "\n",
    "for row in rows:\n",
    "    if row['response'] is not None:\n",
    "        samples_by_dataset_by_sampleix[row['dataset']][row['sample_ix']].append(row)\n",
    "        all_outputs_merged.append(row)\n",
    "\n",
    "unique_preamble_types = set()\n",
    "unique_models = set()\n",
    "\n",
    "sampleixs_to_skip = {}\n",
    "for dataset, samples_by_ix in samples_by_dataset_by_sampleix.items():\n",
    "    print(dataset, ' has ', len(samples_by_ix), ' samples')\n",
    "    sampleixs_to_skip[dataset] = []\n",
    "    prompts_so_far = set()\n",
    "    for sample_ix,row in samples_by_ix.items():\n",
    "        if row[0][PROMPT_KEY] in prompts_so_far:\n",
    "            sampleixs_to_skip[dataset].append(sample_ix)\n",
    "        prompts_so_far.add(row[0][PROMPT_KEY])\n",
    "        unique_preamble_types.update([x['preamble_type'] for x in row])\n",
    "        unique_models.update([x['model'] for x in row])\n",
    "    print(' .. of which ', len(sampleixs_to_skip[dataset]), ' are duplicate')\n",
    "    \n",
    "# Construct the model/preamble pairings to ensure uniform distribution\n",
    "num_outputs = NUM_SAMPLES * len(samples_by_dataset_by_sampleix) * NUM_PAIRS_PER_INSTANCE\n",
    "\n",
    "\n",
    "possible_combos = list(product(list(unique_preamble_types), list(unique_models)))\n",
    "combos_all = []\n",
    "for _ in range(ceil(num_outputs / len(possible_combos))):\n",
    "    np.random.shuffle(possible_combos)\n",
    "    combos_all.extend(possible_combos)\n",
    "\n",
    "\n",
    "combo_groups = [combos_all[i*NUM_PAIRS_PER_INSTANCE:(i+1)*NUM_PAIRS_PER_INSTANCE] for i in range(NUM_SAMPLES * len(samples_by_dataset_by_sampleix)) ]\n",
    "min_set_size = min([len(set(group)) for group in combo_groups])\n",
    "meets_conditions = min_set_size == NUM_PAIRS_PER_INSTANCE\n",
    "if not meets_conditions:\n",
    "    print('Grouping check failed ({:})!'.format(min_set_size))\n",
    "\n",
    "samples_for_prolific = []\n",
    "    \n",
    "batch_ix = 0\n",
    "for dataset, samples_by_ix in samples_by_dataset_by_sampleix.items():\n",
    "    \n",
    "    valid_samples = list(range(0,150))\n",
    "    for invalid_ix in sampleixs_to_skip[dataset]:\n",
    "        valid_samples.remove(invalid_ix)\n",
    "    for ix in valid_samples[OFFSET : OFFSET+NUM_SAMPLES]:\n",
    "        samples = samples_by_ix[ix]\n",
    "        \n",
    "        samples_by_key = {(sample['preamble_type'], sample['model']): sample for sample in samples}\n",
    "        \n",
    "        np.random.shuffle(samples)\n",
    "        \n",
    "#         preambles_to_use = combos_all[batch_ix*NUM_PAIRS_PER_INSTANCE:(batch_ix+1)*NUM_PAIRS_PER_INSTANCE]\n",
    "#         models_to_use = repeated_models[batch_ix*NUM_PAIRS_PER_INSTANCE:(batch_ix+1)*NUM_PAIRS_PER_INSTANCE]\n",
    "#         combo_keys = combos_all[batch_ix*NUM_PAIRS_PER_INSTANCE:(batch_ix+1)*NUM_PAIRS_PER_INSTANCE]\n",
    "        combo_keys = combo_groups[batch_ix]\n",
    "        combos_to_use = [samples_by_key[k] for k in combo_keys]\n",
    "        \n",
    "#         combos = list(combinations(samples[:NUM_PAIRS_PER_INSTANCE], 2))\n",
    "        \n",
    "        combos = list(combinations(combos_to_use, 2))\n",
    "        if NUM_ANNOTATOR_AGREEMENT_SAMPLES > 0:\n",
    "#             dupe_combos = list(combinations(samples[:NUM_PAIRS_PER_INSTANCE], 2))\n",
    "            dupe_combos = list(combinations(combos_to_use, 2))\n",
    "            np.random.shuffle(dupe_combos)\n",
    "            annotator_checks = [dupe_combos[0]] * NUM_ANNOTATOR_AGREEMENT_SAMPLES\n",
    "        else: \n",
    "            annotator_checks = []\n",
    "        for sample_1, sample_2 in combos + annotator_checks:\n",
    "            # switch the order at random\n",
    "            if sample_1[PROMPT_KEY] != sample_2[PROMPT_KEY]:\n",
    "                print('Found mismatched prompts!!')\n",
    "            sample_a, sample_b = (sample_1, sample_2) if random.random() >= 0.5 else (sample_2, sample_1)\n",
    "            assert sample_a[PROMPT_KEY] == sample_b[PROMPT_KEY]\n",
    "#             assert sample_a['preamble'] == sample_b['preamble']\n",
    "            sample_main = [\n",
    "                {'type': 'text','content': '## Prompt\\n-------\\n{:}'.format(normalise_response(sample_a[PROMPT_KEY]))},\n",
    "                 {'type': 'text', 'content': '## Response A\\n-------\\n{:}'.format(normalise_response(sample_a['response']))},\n",
    "                 {'type': 'text', 'content': '## Response B\\n-------\\n{:}'.format(normalise_response(sample_b['response']))}]\n",
    "            \n",
    "            sample_meta = {\n",
    "                'batch_id': BATCH_NAME,\n",
    "                'dataset': dataset,\n",
    "                'model_a': sample_a['model'],\n",
    "                'model_b': sample_b['model'],\n",
    "                'sample_ix': ix,\n",
    "                'preamble_a': sample_a['preamble'],\n",
    "                'preamble_type_a': sample_a['preamble_type'],\n",
    "                'preamble_b': sample_b['preamble'],\n",
    "                'preamble_type_b': sample_b['preamble_type'],\n",
    "            }\n",
    "            \n",
    "            \n",
    "\n",
    "            samples_for_prolific.append(\n",
    "                {\n",
    "                 'text':{\n",
    "                     'Prompt': normalise_response(sample_a[PROMPT_KEY]), \n",
    "                     'Response A': normalise_response(sample_a['response']),\n",
    "                     'Response B': normalise_response(sample_b['response']),\n",
    "                 },\n",
    "                 'meta': sample_meta}\n",
    "            )\n",
    "            \n",
    "        batch_ix += 1\n",
    "            \n",
    "            \n",
    "        # Construct an attention check example\n",
    "\n",
    "        curr_refs = [sample for sample in samples_by_ix[ix] if sample['model'] == REFS_MODEL or not USE_REFS_MODEL_FOR_DISTRACTORS]\n",
    "        np.random.shuffle(curr_refs)\n",
    "        if len(curr_refs) == 0:\n",
    "            continue\n",
    "        sample_1 = curr_refs[0]\n",
    "        distractor_ix = np.random.choice([i for i in samples_by_ix.keys() if i != ix])\n",
    "        sample_2_options = [sample for sample in samples_by_ix[distractor_ix] if sample['model'] == sample_1['model']]\n",
    "        np.random.shuffle(sample_2_options)\n",
    "        sample_2 = sample_2_options[0]\n",
    "        sample_2 = {**sample_2, 'model':'distractor'}\n",
    "        sample_a, sample_b = (sample_1, sample_2) if random.random() >= 0.5 else (sample_2, sample_1)\n",
    "\n",
    "        sample_main = [\n",
    "            {'type': 'text','content': '## Prompt\\n-------\\n{:}'.format(normalise_response(sample_1[PROMPT_KEY]))},\n",
    "             {'type': 'text', 'content': '## Response A\\n-------\\n{:}'.format(normalise_response(sample_a['response']))},\n",
    "             {'type': 'text', 'content': '## Response B\\n-------\\n{:}'.format(normalise_response(sample_b['response']))}]\n",
    "\n",
    "        sample_meta = {\n",
    "            'batch_id': BATCH_NAME,\n",
    "            'dataset': dataset,\n",
    "            'model_a': sample_a['model'],\n",
    "            'model_b': sample_b['model'],\n",
    "            'sample_ix': 'distractor_' + str(ix),\n",
    "            'preamble_a': sample_a['preamble'],\n",
    "            'preamble_type_a': sample_a['preamble_type'],\n",
    "            'preamble_b': sample_b['preamble'],\n",
    "            'preamble_type_b': sample_b['preamble_type'],\n",
    "        }\n",
    "\n",
    "\n",
    "        samples_for_prolific.append(\n",
    "            {\n",
    "             'text':{\n",
    "                 'Prompt': normalise_response(sample_1[PROMPT_KEY]), \n",
    "                 'Response A': normalise_response(sample_a['response']),\n",
    "                 'Response B': normalise_response(sample_b['response']),\n",
    "             },\n",
    "             'meta': sample_meta}\n",
    "        )\n",
    "\n",
    "            \n",
    "# Shuffle, then assign IDs, so that potato displays them in shuffled order\n",
    "np.random.shuffle(samples_for_prolific)\n",
    "samples_for_prolific = [{'id': str(i), **x} for i, x in enumerate(samples_for_prolific)]\n",
    "\n",
    "\n",
    "with jsonlines.open(f'../results/batch_controlled/prolific_{BATCH_NAME}.jsonl','w') as writer:\n",
    "    writer.write_all(samples_for_prolific)\n",
    "\n",
    "len(samples_for_prolific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "068465f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'complexity_low': 36,\n",
       "          'confidence_low': 30,\n",
       "          'normal': 29,\n",
       "          'complexity_high': 28,\n",
       "          'confidence_high': 27}),\n",
       " Counter({'distractor': 75,\n",
       "          'command_6Bpref_v14.7_20230817': 21,\n",
       "          'llama13chat': 18,\n",
       "          'falcon40': 16,\n",
       "          'command_52Bpref_v14.7_20230817': 11,\n",
       "          'mpt30instruct': 9}))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check distractor counts\n",
    "\n",
    "distractor_preambles = Counter()\n",
    "distractor_models = Counter()\n",
    "for row in samples_for_prolific:\n",
    "    if 'distractor' in str(row['meta']['sample_ix']):\n",
    "#         print(row['meta'])\n",
    "        distractor_preambles[row['meta']['preamble_type_a']] += 1\n",
    "        distractor_preambles[row['meta']['preamble_type_b']] += 1\n",
    "        distractor_models[row['meta']['model_a']] += 1\n",
    "        distractor_models[row['meta']['model_b']] += 1\n",
    "        \n",
    "distractor_preambles, distractor_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3a7f239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'wikihow': 275, 'amazon': 275, 'curation': 275})\n",
      "Counter({'falcon40': 300, 'mpt30instruct': 300, 'command_52Bpref_v14.7_20230817': 300, 'command_6Bpref_v14.7_20230817': 300, 'llama13chat': 300})\n",
      "Counter({'Respond in a cautious, defensive and uncertain way, as if you are unfamilar with the topic.': 300, 'Respond using complex language, long words and technical terms, as if you are an expert.': 300, 'Respond authoritatively, assertively and persuasively, as if you are very knowledgeable about the topic.': 300, '': 300, 'Respond using only short words and simple language, as if you were talking to a child.': 300})\n",
      "Counter({'confidence_low': 300, 'complexity_high': 300, 'confidence_high': 300, 'normal': 300, 'complexity_low': 300})\n",
      "Max prompt count:  11\n"
     ]
    }
   ],
   "source": [
    "# Check other counts are as expected\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "dataset_counter = Counter()\n",
    "\n",
    "for row in samples_for_prolific:\n",
    "    dataset_counter[row['meta']['dataset']] += 1\n",
    "    \n",
    "# Expected: 8n for factual nlg, 13n for others (4C2+2, 5C2+1 + 2) (combos + distractor + agreement checks)\n",
    "print(dataset_counter)\n",
    "\n",
    "model_counter = Counter()\n",
    "\n",
    "for row in samples_for_prolific:\n",
    "    if 'distractor' not in [row['meta']['model_a'],row['meta']['model_b']]:\n",
    "        model_counter[row['meta']['model_a']] += 1\n",
    "        model_counter[row['meta']['model_b']] += 1\n",
    "    \n",
    "# Expected: 8n for factual nlg, 13n for others (4C2+2, 5C2+1 + 2) (combos + distractor + agreement checks)\n",
    "print(model_counter)\n",
    "\n",
    "preamble_type_counter = Counter()\n",
    "preamble_counter = Counter()\n",
    "\n",
    "for row in samples_for_prolific:\n",
    "    if 'distractor' not in [row['meta']['model_a'],row['meta']['model_b']]:\n",
    "        preamble_type_counter[row['meta']['preamble_type_a']] += 1\n",
    "        preamble_type_counter[row['meta']['preamble_type_b']] += 1\n",
    "        preamble_counter[row['meta']['preamble_a']] += 1\n",
    "        preamble_counter[row['meta']['preamble_b']] += 1\n",
    "    \n",
    "# Expected: 8n for factual nlg, 13n for others (4C2+2, 5C2+1 + 2) (combos + distractor + agreement checks)\n",
    "print(preamble_counter)\n",
    "print(preamble_type_counter)\n",
    "\n",
    "\n",
    "prompt_counter = Counter()\n",
    "\n",
    "for row in samples_for_prolific:\n",
    "    prompt_counter[row['text']['Prompt']] += 1\n",
    "    \n",
    "# Expected: (6+agreement)n for factual nlg, (11+agreement)n for others (4C2, 5C2+1) (combos + distractor + agreement checks)\n",
    "print('Max prompt count: ', prompt_counter.most_common(1)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "36844495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), 95, 100, 100, 100)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check total prompt counts are as expected\n",
    "\n",
    "with jsonlines.open('../results/batch_v1/prolific_main_batch_1_repeats.jsonl') as reader:\n",
    "    batch_1 = list(reader)\n",
    "with jsonlines.open('../results/batch_v1/prolific_main_batch_2.jsonl') as reader:\n",
    "    batch_2 = list(reader)\n",
    "with jsonlines.open('../results/batch_v1/prolific_main_batch_3.jsonl') as reader:\n",
    "    batch_3 = list(reader)\n",
    "with jsonlines.open('../results/batch_v1/prolific_main_batch_4.jsonl') as reader:\n",
    "    batch_4 = list(reader)\n",
    "    \n",
    "batch_1_prompts = set([row['text']['Prompt'] for row in batch_1])\n",
    "batch_2_prompts = set([row['text']['Prompt'] for row in batch_2])\n",
    "batch_3_prompts = set([row['text']['Prompt'] for row in batch_3])\n",
    "batch_4_prompts = set([row['text']['Prompt'] for row in batch_4])\n",
    "\n",
    "batch_1_prompts & batch_2_prompts & batch_3_prompts & batch_4_prompts, len(batch_1_prompts), len(batch_2_prompts), len(batch_3_prompts), len(batch_4_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0415d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cfb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d71385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented responses from Cohere models\n",
    "\n",
    "import cohere\n",
    "from cohere import CohereAPIError, CohereConnectionError, CohereError\n",
    "\n",
    "API_KEY = os.environ.get('COHERE_API_KEY')\n",
    "\n",
    "  \n",
    "co = cohere.Client(API_KEY)\n",
    "\n",
    "# preambles_complexity = [\n",
    "#     ('complexity_high', 'Respond using jargon, long words and technical language appropriate for an expert.'),\n",
    "#     ('normal', ''),\n",
    "#     ('complexity_low', 'Respond using only short words and simple language appropriate for a child.')\n",
    "    \n",
    "# ]\n",
    "preambles_complexity = [\n",
    "    ('complexity_high', 'Respond using additional jargon, long words and technical terms, as if you are an expert addressing another expert.'),\n",
    "    ('normal', ''),\n",
    "    ('complexity_low', 'Explain to me like I\\'m five, and respond using only short words and simple language, as if you were talking to a child.')\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# preambles_confidence = [\n",
    "#     ('confidence_high','Respond in an persuasive, assertive and confident way.'),\n",
    "#     ('normal',''),\n",
    "#     ('confidence_low','Respond in an cautious and balanced way, including both sides of the argument.'),\n",
    "# ]\n",
    "preambles_confidence = [\n",
    "    ('confidence_high','Respond aggressively, assertively and confidently, as if you were a genius talking to a stupid person.'),\n",
    "    ('normal',''),\n",
    "    ('confidence_low','Respond in a very cautious and uncertain way, as if you were an idiot talking to a genius.'),\n",
    "]\n",
    "\n",
    "all_responses = []\n",
    "\n",
    "\n",
    "for _, preamble in preambles_confidence:\n",
    "\n",
    "\n",
    "    responses_command = co.batch_generate(\n",
    "        model='command-nightly',\n",
    "#         prompts=['User: ' + p['prompt'] +'\\n' + preamble + '\\nChatbot:'.strip() for p in wikihow_prompts],\n",
    "        prompts=[p['prompt'] +'\\n' + preamble for p in wikihow_prompts],\n",
    "        return_exceptions=True,\n",
    "        max_tokens=500,\n",
    "        temperature=0.7,\n",
    "        num_generations=1,\n",
    "    )\n",
    "\n",
    "    all_responses.append([[(r.text if not isinstance(resp, CohereError) else None) for r in resp] for resp in responses_command])\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32394685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open('../results/batch_v1/prompts_merged.jsonl') as reader:\n",
    "    prompts = list(reader)\n",
    "\n",
    "len(prompts)\n",
    "confound_prompts = [p for p in prompts if p['dataset'] in ['wikihow','curation','amazon']]\n",
    "\n",
    "prompts_by_dataset = defaultdict(list)\n",
    "\n",
    "for prompt in confound_prompts:\n",
    "    prompts_by_dataset[prompt['dataset']].append(prompt)\n",
    "\n",
    "confound_prompts = []\n",
    "for prompts in prompts_by_dataset.values():\n",
    "    confound_prompts.extend(prompts[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76f7c54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confound_prompts_augmented = []\n",
    "\n",
    "# preambles_complexity = [\n",
    "#     ('complexity_high', 'Respond using jargon, long words and technical language appropriate for an expert.'),\n",
    "#     ('normal', ''),\n",
    "#     ('complexity_low', 'Respond using only short words and simple language appropriate for a child.')\n",
    "    \n",
    "# ]\n",
    "# preambles_complexity = [\n",
    "#     ('complexity_high', 'Respond using additional jargon, long words and technical terms, as if you are an expert addressing another expert.'),\n",
    "#     ('normal', ''),\n",
    "#     ('complexity_low', 'Explain to me like I\\'m five, and respond using only short words and simple language, as if you were talking to a child.')\n",
    "    \n",
    "# ]\n",
    "preambles_complexity = [\n",
    "    ('complexity_high', 'Respond using complex language, long words and technical terms, as if you are an expert.'),\n",
    "    ('normal', ''),\n",
    "    ('complexity_low', 'Respond using only short words and simple language, as if you were talking to a child.')\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# preambles_confidence = [\n",
    "#     ('confidence_high','Respond in an persuasive, assertive and confident way.'),\n",
    "#     ('normal',''),\n",
    "#     ('confidence_low','Respond in an cautious and balanced way, including both sides of the argument.'),\n",
    "# ]\n",
    "# preambles_confidence = [\n",
    "#     ('confidence_high','Respond aggressively, assertively and confidently, as if you were a genius talking to a stupid person.'),\n",
    "#     ('normal',''),\n",
    "#     ('confidence_low','Respond in a very cautious and uncertain way, as if you were an idiot talking to a genius.'),\n",
    "# ]\n",
    "\n",
    "preambles_confidence = [\n",
    "    ('confidence_high','Respond authoritatively, assertively and persuasively, as if you are very knowledgeable about the topic.'),\n",
    "    ('normal',''),\n",
    "    ('confidence_low','Respond in a cautious, defensive and uncertain way, as if you are unfamilar with the topic.'),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Respond authoritatively, assertively and persuasively, as if you were an expert.\n",
    "# Respond in a very cautious and uncertain way, as if you are unfamilar with the topic.\n",
    "\n",
    "# Respond using complex language, long words and technical terms, as if you are an expert.\n",
    "# Explain to me like I\\'m five, and respond using only short words and simple language, as if you were talking to a child.\n",
    "\n",
    "preambles = {x[0]: x[1] for x in preambles_complexity+preambles_confidence}\n",
    "\n",
    "def prompt_to_augmented(orig, augment, dataset):\n",
    "    if dataset == 'curation':\n",
    "        if '\\nGenerate a summary:' in orig:\n",
    "            return orig.replace('\\nGenerate a summary:', '\\n' + augment + ' Generate a summary:')\n",
    "        else:\n",
    "            return orig + '\\n' + augment\n",
    "    elif dataset == 'amazon':\n",
    "        if '\\nProduct description:' in orig:\n",
    "            return orig.replace('\\nProduct description:', '\\n' + augment + '\\nProduct description:')\n",
    "        else:\n",
    "            return orig + '\\n' + augment\n",
    "    else:\n",
    "        return orig + '\\n' + augment\n",
    "\n",
    "for preamble_type, preamble_text in preambles.items():\n",
    "    \n",
    "    if 'complexity' in preamble_type:\n",
    "        for prompt in confound_prompts * 0:\n",
    "            confound_prompts_augmented.append({**prompt, 'original_prompt': prompt['prompt'], 'preamble': preamble_text, 'prompt': prompt_to_augmented(prompt['prompt'], preamble_text,prompt['dataset']), 'preamble_type': preamble_type })\n",
    "    elif 'confidence' in preamble_type:\n",
    "        for prompt in confound_prompts * 2:\n",
    "            confound_prompts_augmented.append({**prompt, 'original_prompt': prompt['prompt'], 'preamble': preamble_text, 'prompt': prompt_to_augmented(prompt['prompt'], preamble_text,prompt['dataset']), 'preamble_type': preamble_type })\n",
    "    \n",
    "    else:\n",
    "        for prompt in confound_prompts * 0:\n",
    "            confound_prompts_augmented.append({**prompt, 'original_prompt': prompt['prompt'], 'preamble': preamble_text, 'prompt': prompt_to_augmented(prompt['prompt'], preamble_text,prompt['dataset']), 'preamble_type': preamble_type })\n",
    "        \n",
    "        \n",
    "len(confound_prompts_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05577bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'amazon',\n",
       " 'prompt': 'Product name:\\nWayne SWS50-12P 1/2 hp Shallow Well Jet Pump Conventional 12 gallon Tank System\\n\\nProduct keywords:\\n1/2 hp cast iron shallow well jet pump conventional tank system with 12 gallon horizontal tank\\n1/2 hp; max. Flow rate is 420 gallons per hour; (50 psi)\\n3/4 in. Npt discharge with 1-1/4 in. Suction\\nDual voltage (120V/240V) high efficiency square flange motor (factory set for 240V)\\nPressure switch is pre-set at 30-50 psi for automatic operation\\nAssembled in US with foreign and domestic parts\\nDurable cast iron volute with dedicated priming port for volute filling. For wells 0 ft. - 25 ft. in depth.\\n\\nRespond authoritatively, assertively and persuasively, as if you are very knowledgeable about the topic.\\nProduct description:',\n",
       " 'sample_ix': 51,\n",
       " 'original_prompt': 'Product name:\\nWayne SWS50-12P 1/2 hp Shallow Well Jet Pump Conventional 12 gallon Tank System\\n\\nProduct keywords:\\n1/2 hp cast iron shallow well jet pump conventional tank system with 12 gallon horizontal tank\\n1/2 hp; max. Flow rate is 420 gallons per hour; (50 psi)\\n3/4 in. Npt discharge with 1-1/4 in. Suction\\nDual voltage (120V/240V) high efficiency square flange motor (factory set for 240V)\\nPressure switch is pre-set at 30-50 psi for automatic operation\\nAssembled in US with foreign and domestic parts\\nDurable cast iron volute with dedicated priming port for volute filling. For wells 0 ft. - 25 ft. in depth.\\n\\nProduct description:',\n",
       " 'preamble': 'Respond authoritatively, assertively and persuasively, as if you are very knowledgeable about the topic.',\n",
       " 'preamble_type': 'confidence_high'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confound_prompts_augmented[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51b83fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../results/batch_controlled/prompts_augmented_v2_part2.jsonl','w') as writer:\n",
    "    writer.write_all(confound_prompts_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff369f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command52Bpref 3900\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_52Bpref_v14.7_20230817\n",
      "command6Bpref 3900\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "All candidates included sorry! command_6Bpref_v14.7_20230817\n",
      "falcon40 3900\n",
      "All candidates included sorry! falcon40\n",
      "All candidates included sorry! falcon40\n",
      "All candidates included sorry! falcon40\n",
      "All candidates included sorry! falcon40\n",
      "All candidates included sorry! falcon40\n",
      "All candidates included sorry! falcon40\n",
      "All candidates included sorry! falcon40\n",
      "llama13chat 3900\n",
      "mpt30instruct 3900\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "# Now we need to rerank the complexity samples\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from readability import Readability\n",
    "\n",
    "systems = [\n",
    "#     'command52B',\n",
    "#     'command6B',\n",
    "    \n",
    "    \n",
    "#     'refs',\n",
    "#     'vicuna30uncensored',\n",
    "#     'mpt7',\n",
    "    \n",
    "    'command52Bpref',\n",
    "    'command6Bpref',\n",
    "    'falcon40',\n",
    "    'llama13chat',\n",
    "    'mpt30instruct',\n",
    "]\n",
    "\n",
    "reranked_outputs = []\n",
    "\n",
    "for system in systems:\n",
    "    with jsonlines.open(f'../results/batch_controlled/output_augmented_v2_{system}.jsonl') as reader:\n",
    "        outputs = list(reader)\n",
    "    with jsonlines.open(f'../results/batch_controlled/output_augmented_v2_part2_{system}.jsonl') as reader:\n",
    "        outputs += list(reader)\n",
    "        \n",
    "    print(system, len(outputs))\n",
    "        \n",
    "    outputs = [{**row, 'model': system if 'model' not in row else row['model']} for row in outputs]\n",
    "        \n",
    "    outputs_by_preamble_by_sample = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for output in outputs:\n",
    "        outputs_by_preamble_by_sample[output['preamble_type']][(output['dataset'], output['sample_ix'] )].append(output)\n",
    "        \n",
    "    \n",
    "    for preamble_type in ['complexity_high', 'complexity_low']:\n",
    "        for responses in outputs_by_preamble_by_sample[preamble_type].values():\n",
    "            scores = []\n",
    "            for response in responses:\n",
    "                if response['response'] is None:\n",
    "                    scores.append(-100)\n",
    "                    continue\n",
    "        #         print(response)\n",
    "                r = Readability(response['response'])\n",
    "                if len(word_tokenize(response['response'])) < 10 or r.statistics()['num_words'] < 100:\n",
    "                    scores.append(-100)\n",
    "                    continue\n",
    "            \n",
    "            \n",
    "                \n",
    "        #         score = r.flesch_kincaid().grade_level\n",
    "                score = r.dale_chall().score\n",
    "                scores.append(score)\n",
    "\n",
    "            if len(scores) > 0:\n",
    "                if preamble_type[-5:] == '_high':\n",
    "                    reranked_outputs.append(responses[np.argmax(scores)])\n",
    "                if preamble_type[-4:] == '_low':\n",
    "                    reranked_outputs.append(responses[np.argmin(scores)])\n",
    "\n",
    "                    \n",
    "    for preamble_type in ['confidence_high', 'confidence_low']:\n",
    "        \n",
    "        for responses in outputs_by_preamble_by_sample[preamble_type].values():\n",
    "#             print(len(responses))\n",
    "            candidates = []\n",
    "            \n",
    "            nosorry_responses = [r for r in responses if (r['response'] is not None and \"I'm sorry\" not in r['response'])]\n",
    "            if len(nosorry_responses) > 0:\n",
    "                reranked_outputs.append(nosorry_responses[0]) \n",
    "            else:\n",
    "                print('All candidates included sorry!', responses[0]['model'])\n",
    "                reranked_outputs.append(responses[0]) \n",
    "                    \n",
    "    for responses in outputs_by_preamble_by_sample['normal'].values():\n",
    "        if len(responses) > 1:\n",
    "            print('Duplicate responses for normal?!')\n",
    "        \n",
    "        reranked_outputs.append(responses[0])\n",
    "        \n",
    "                   \n",
    "# Length should be n * 5 models * 3 tasks * 5 groups\n",
    "print(len(reranked_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "20edcffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../results/batch_controlled/output_augmented_merged.jsonl','w') as writer:\n",
    "    writer.write_all(reranked_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426293fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "4cc12303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the batches for expert annotation\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "BATCH_NAME = 'confound_validation_1'\n",
    "\n",
    "with jsonlines.open('../results/batch_controlled/prolific_confound_main_1_agreement.jsonl') as reader:\n",
    "    orig_inputs = list(reader)\n",
    "    \n",
    "orig_inputs_by_key = {}\n",
    "for row in orig_inputs:\n",
    "    key_a = (row['meta']['sample_ix'], row['meta']['dataset'],row['meta']['model_a'],row['meta']['preamble_type_a'],)\n",
    "    if key_a not in orig_inputs_by_key and 'distractor' not in str(row['meta']['sample_ix']):\n",
    "        orig_inputs_by_key[key_a] = {\n",
    "            'text': {'Prompt': row['text']['Prompt'], 'Response A': row['text']['Response A']},\n",
    "            'meta': {\n",
    "                'batch_id': BATCH_NAME,\n",
    "                'dataset': row['meta']['dataset'],\n",
    "                'sample_ix': row['meta']['sample_ix'],\n",
    "                'model_a': row['meta']['model_a'],\n",
    "                'preamble_type_a': row['meta']['preamble_type_a'],\n",
    "                'preamble_a': row['meta']['preamble_a'],\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    key_b = (row['meta']['sample_ix'], row['meta']['dataset'], row['meta']['model_b'],row['meta']['preamble_type_b'],)\n",
    "    if key_b not in orig_inputs_by_key and 'distractor' not in str(row['meta']['sample_ix']):\n",
    "        orig_inputs_by_key[key_b] = {\n",
    "            'text': {'Prompt': row['text']['Prompt'], 'Response A': row['text']['Response B']},\n",
    "            'meta': {\n",
    "                'batch_id': BATCH_NAME,\n",
    "                'dataset': row['meta']['dataset'],\n",
    "                'sample_ix': row['meta']['sample_ix'],\n",
    "                'model_a': row['meta']['model_b'],\n",
    "                'preamble_type_a': row['meta']['preamble_type_b'],\n",
    "                'preamble_a': row['meta']['preamble_b'],\n",
    "            },\n",
    "        }\n",
    "        \n",
    "validation_batch = [{'id': str(i), **row[1]} for i, row in enumerate(sorted(orig_inputs_by_key.items(), key=lambda x: x[0]))]\n",
    "\n",
    "with jsonlines.open('../results/batch_controlled/prolific_confound_validation_1.jsonl' ,'w') as writer:\n",
    "    writer.write_all(validation_batch[:150])\n",
    "with jsonlines.open('../results/batch_controlled/prolific_confound_validation_2.jsonl' ,'w') as writer:\n",
    "    writer.write_all(validation_batch[150:300])\n",
    "with jsonlines.open('../results/batch_controlled/prolific_confound_validation_3.jsonl' ,'w') as writer:\n",
    "    writer.write_all(validation_batch[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "c49531d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_inputs_by_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb4dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cf978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
