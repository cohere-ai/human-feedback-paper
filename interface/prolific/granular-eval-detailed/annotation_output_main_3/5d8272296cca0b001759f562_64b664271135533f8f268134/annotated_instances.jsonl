{"id": "74", "input_text": {"Prompt": "Conversation:\nJane: Honey, I've got good news for you.\nSherlock: What is it?\nJane: You are going to be a father.\nSherlock: What? Do you mean that you are pregnant? Is it true?\nJane: Yes. We'll have our baby soon.\nSherlock: Oh, dear, I'm so happy.\nJane: Me, too.\n\nNow write a summary of the conversation:", "Response A": "Jane and Sherlock are going to have a baby soon.", "Response B": "Jane is pregnant and going to have a baby. Sherlock is very happy about it."}, "input_meta": {"batch_id": "main_batch_3", "dataset": "dialogsum", "model_a": "command_52B_v14_20230622", "model_b": "command_6B_v14_20230622", "sample_ix": 63}, "label_annotations": {"email": "5d8272296cca0b001759f562_64b664271135533f8f268134", "src": "next_instance", "instance_id": "0", "style_a": "2", "style_b": "3", "confidence_a": "3", "confidence_b": "3", "useful_a": "3", "useful_b": "3", "detail_a": "2", "detail_b": "3", "clarity_a": "3", "clarity_b": "4", "creativity_a": "2", "creativity_b": "3", "positive_val": "5", "negative_val": "1", "inconsistent_a": "No", "inconsistent_b": "No", "contradiction_a": "No", "contradiction_b": "No", "factuality_a": "No", "factuality_b": "No", "relevance_a": "No", "relevance_b": "No", "formatting_a": "No", "formatting_b": "No", "scope_a": "No", "scope_b": "No", "refusal_a": "No", "refusal_b": "No", "fluency_a": "No", "fluency_b": "No", "harmful_a": "No", "harmful_b": "No", "repetition_a": "No", "repetition_b": "No", "overall_a": "2", "overall_b": "3", "factors_a": "no", "factors_b": "no", "feedback": "both responses were adequate but B was more helpful.  I didn't find the scenario to be particularly comfortable (felt disjointed) so this may well have led to the quality of the responses being compromised"}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 58s "}}
