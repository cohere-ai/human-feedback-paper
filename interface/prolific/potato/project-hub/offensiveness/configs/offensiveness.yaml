{
    "port": 9001,

    "server_name": "potato annotator",

    "annotation_task_name": "Offensiveness",

    # Potato will write the annotation file for all annotations to this
    # directory, as well as per-annotator output files and state information
    # necessary to restart annotation.
    "output_annotation_dir": "annotation_output/full/",

    # The output format for the all-annotator data. Allowed formats are:
    # * jsonl
    # * json (same output as jsonl)
    # * csv
    # * tsv
    #
    "output_annotation_format": "tsv",

    # If annotators are using a codebook, this will be linked at the top to the
    # instance for easy access
    "annotation_codebook_url": "",

    "data_files": [
       "data_files/ruddit_1500.csv"
    ],

    "item_properties": {
        "id_key": "id",
        "text_key": "text",
    },

  #list_as_text is used when the input text is actually a list of texts, usually used for best-worst-scaling
    "list_as_text": {
      "text_list_prefix_type": 'None',
      "horizontal": True,
    },

    "user_config": {

      "allow_all_users": True,

      "users": [  ],
    },

    #defining the ways annotators entering the annotation system
    "login": {
       "type": 'url_direct',    #can be 'password' or 'url_direct'
       "url_argument": 'PROLIFIC_PID' # when the login type is set to 'url_direct', 'url_argument' must be setup for a direct url argument login
    },

    #the jumping-to-id function will be disabled if "jumping_to_id_disabled" is True
    "jumping_to_id_disabled": True,

  #the navigation bar will be hidden to the annotators if "hide_navbar" is True
    "hide_navbar": True,

  # define the surveyflow of the system, set up the pages before and after the data annotation page
    "surveyflow": {
      "on": True,
      #"order" : ['pre_annotation', 'prestudy_passed', 'prestudy_failed', 'post_annotation'],
      "order" : ['pre_annotation', 'post_annotation'],
      "pre_annotation": ['surveyflow/intro.jsonl', 'surveyflow/instruction.jsonl', 'surveyflow/consent.jsonl'],
      "prestudy_passed": ['surveyflow/prestudy_pass.jsonl'],
      "prestudy_failed": ['surveyflow/prestudy_fail.jsonl'],
      "post_annotation": ['surveyflow/experience.jsonl', 'surveyflow/demographic_questions.jsonl','surveyflow/end.jsonl'],
      # If set, we will automatically generate testing questions similar to the annotation instances, but explicitly ask the annotator to choose one option
      "testing": [],
    },

    #prestudy test, annotators who fail this test will be disalloed to continue annotation
    "prestudy": {
        "on": False,
        "minimum_score": 0.8,
        "groundtruth_key": 'whether_match',
        "question_key": 'Whether the presented sentences are discussing the same scientific finding',
        "answer_mapping": {'Yes': True, 'No': False},
        "pass_page": 'surveyflow/prestudy_pass.jsonl',
        "fail_page": 'surveyflow/prestudy_fail.jsonl'
    },


    "automatic_assignment": {
      #whether do automatic task assignment for annotators, default False.
      "on": True,
      "output_filename": 'task_assignment.json',
      "sampling_strategy": 'random',
      "labels_per_instance": 10,
      "instance_per_annotator": 50,
      "test_question_per_annotator": 0, # you must set up the test question in surveyflow to use this function

      "users": [  ],
    },


    # How many seconds do you want the annotators spend on each instance, after
    # that, an alert will be sent per alert_time_each_instance seconds.
    "alert_time_each_instance": 10000000,
    "horizontal_key_bindings": true,

    "annotation_schemes": [
      {
        "annotation_type": "likert",

        # This name gets used in reporting the annotation results
        "name": "Consider you read the above comment on Reddit, how offensive do you think it is?",

        # This text is shown to the user and can be a longer statement
        "description": "Consider you read the above comment on Reddit, how offensive do you think it is?",

        # The min and max labels are text shown at each end of the scale
        "min_label": "Not offensive at all",
        "max_label": "Very offensive",
        # adding requirements for labels, when "required" is True, the annotators will be asked to finish the current instance to proceed
        "label_requirement": { "required": True },
        # How many scale points to show
        "size": 5,

        # If true, keys [1-size] will be bound to scale responses. Likert
        # scales larger than 10 are not supported with this simple
        # keybinding and will need to use the full item specification to
        # bind all scale points to keys.
        "sequential_key_binding": True,
      }
    ],

    # The html that changes the visualiztation for your task. Change this file
    # to influence the layout and description of your task. This is not a full
    # HTML page, just the piece that does lays out your task's pieces
    #"html_layout": "templates/examples/fixed_keybinding_layout.html",
    "html_layout": "default",

    # The core UI files for Potato. You should not need to change these normally.
    #
    # Exceptions to this might include:
    # 1) You want to add custom CSS/fonts to style your task
    # 2) Your layout requires additional JS/assets to render
    # 3) You want to support additional keybinding magic
    #
    # if you want to use your own template,
    # please replace the string as a path to the template
    "base_html_template": "default",
    "header_file": "default",

    # This is where the actual HTML files will be generated
    "site_dir": "default"

}